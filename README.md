# Hand Gesture Recognition Model

A complete machine learning pipeline for building a custom hand detection and gesture recognition model (similar to MediaPipe) and applying it to control Spotify playback.

## ğŸ¯ Project Goals

This project demonstrates the full ML workflow:
1. **Data Collection**: Gather hand gesture data from webcam
2. **Model Development**: Build and train a custom hand detection model
3. **Model Training**: Train the model on collected data
4. **Application**: Deploy the model in a real-world application (Spotify controller)

## ğŸ“ Project Structure

```
hand-gesture-model/
â”‚
â”œâ”€â”€ 1-data-collection/         # Phase 1: Data Collection
â”‚   â”œâ”€â”€ collect_data.py       # Collect hand gesture images
â”‚   â”œâ”€â”€ annotate_data.py      # Annotate data with landmarks
â”‚   â””â”€â”€ README.md             # Data collection guide
â”‚
â”œâ”€â”€ data/                      # Dataset storage
â”‚   â”œâ”€â”€ raw/                  # Raw collected images
â”‚   â”œâ”€â”€ processed/            # Preprocessed data ready for training
â”‚   â”œâ”€â”€ annotations/          # Annotated landmark data
â”‚   â””â”€â”€ kaggle_data_link.md   # External dataset links
â”‚
â”œâ”€â”€ 2-model-development/       # Phase 2: Model Building & Training
â”‚   â”œâ”€â”€ model_architecture.py # Neural network architecture
â”‚   â”œâ”€â”€ train.py              # Model training script
â”‚   â”œâ”€â”€ evaluate.py           # Model evaluation script
â”‚   â”œâ”€â”€ data_preprocessing.py # Data preprocessing utilities
â”‚   â””â”€â”€ README.md             # Model development guide
â”‚
â”œâ”€â”€ models/                    # Trained models
â”‚   â”œâ”€â”€ saved_models/         # Final trained models
â”‚   â””â”€â”€ checkpoints/          # Training checkpoints
â”‚
â”œâ”€â”€ 3-application/             # Phase 3: Real-world Application
â”‚   â”œâ”€â”€ main.py               # Main application entry point
â”‚   â”œâ”€â”€ hand_detector.py      # Hand detection using trained model
â”‚   â”œâ”€â”€ spotify_controller.py # Spotify API integration
â”‚   â”œâ”€â”€ gesture_mapping.py    # Gesture to action mapping
â”‚   â”œâ”€â”€ utils.py              # Application utilities
â”‚   â””â”€â”€ README.md             # Application usage guide
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter notebooks for experimentation
â”‚
â”œâ”€â”€ tests/                     # Unit tests
â”‚   â”œâ”€â”€ test_hand_detector.py
â”‚   â””â”€â”€ test_spotify_controller.py
â”‚
â”œâ”€â”€ config/                    # Configuration files
â”‚   â”œâ”€â”€ config.py             # Main configuration
â”‚   â””â”€â”€ config.example.py     # Example configuration
â”‚
â”œâ”€â”€ assets/                    # Images, icons, and other assets
â”‚
â”œâ”€â”€ docs/                      # Additional documentation
â”‚
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸš€ Quick Start

### Phase 1: Data Collection
```bash
cd 1-data-collection
python collect_data.py      # Collect hand gesture images
python annotate_data.py     # Annotate with landmarks
```

### Phase 2: Model Development
```bash
cd 2-model-development
python data_preprocessing.py   # Preprocess data
python train.py               # Train the model
python evaluate.py            # Evaluate performance
```

### Phase 3: Application
```bash
cd 3-application
python main.py               # Run Spotify controller
```

See individual README files in each phase directory for detailed instructions.

## ğŸ“‹ Prerequisites

- Python 3.8+
- Webcam for data collection and real-time detection
- Spotify Premium account (for Phase 3 application)
- GPU recommended for faster training (optional)

## ğŸ”§ Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd spotify-hand-controller-built-mediapipe-model
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure Spotify API** (for Phase 3)
```bash
cp config/config.example.py config/config.py
# Edit config/config.py with your Spotify API credentials
```

Get Spotify API credentials:
- Go to [Spotify Developer Dashboard](https://developer.spotify.com/dashboard)
- Create a new app
- Copy Client ID and Client Secret to config.py

## ğŸ“š Complete Workflow

### Phase 1: Data Collection
Collect your own hand gesture dataset:

```bash
cd 1-data-collection

# Step 1: Collect images
python collect_data.py
# Follow on-screen instructions to record gestures

# Step 2: Annotate landmarks
python annotate_data.py
# Extracts hand landmarks using MediaPipe
```

**Output**: Annotated data in `data/annotations/`

### Phase 2: Model Development & Training
Build and train your custom model:

```bash
cd 2-model-development

# Step 1: Preprocess data
python data_preprocessing.py
# Creates train/val/test splits with augmentation

# Step 2: Train model
python train.py --epochs 100 --batch-size 32

# Step 3: Evaluate model
python evaluate.py --model-path ../models/saved_models/best_model.h5
```

**Output**: Trained model in `models/saved_models/`

### Phase 3: Application (Spotify Controller)
Use your trained model to control Spotify:

```bash
cd 3-application

# Run the application
python main.py --model-path ../models/saved_models/best_model.h5
```

**Default Gesture Mappings**:
- ğŸ–ï¸ **Open Palm**: Play/Pause
- âœŠ **Fist**: Stop
- ğŸ‘ **Thumbs Up**: Volume Up
- âœŒï¸ **Peace Sign**: Next Track
- ğŸ‘ˆ **Pointing**: Previous Track
- â† **Swipe Left**: Seek Backward
- â†’ **Swipe Right**: Seek Forward

Customize mappings in [3-application/gesture_mapping.py](3-application/gesture_mapping.py)

## ğŸ“ Learning Path

This project is designed to teach the complete ML pipeline:

1. **Data Engineering**: Learn data collection, annotation, and preprocessing
2. **Model Architecture**: Understand CNN architectures for computer vision
3. **Training Pipeline**: Implement training loops, checkpointing, and evaluation
4. **Model Deployment**: Deploy ML models in real-time applications
5. **API Integration**: Integrate with external APIs (Spotify)

## ğŸ§ª Testing

Run tests to verify components:

```bash
# Test all components
pytest tests/

# Test specific component
pytest tests/test_hand_detector.py
pytest tests/test_spotify_controller.py
```

## ğŸ“Š Model Architecture

Our custom model is inspired by MediaPipe and consists of:

1. **Hand Detection Network**: Locates hands in images using MobileNetV2 backbone
2. **Landmark Prediction Network**: Predicts 21 3D hand landmarks
3. **Gesture Classification Network**: Classifies gestures from landmarks

See [2-model-development/model_architecture.py](2-model-development/model_architecture.py) for details.

## ğŸ¯ Performance Targets

- **Hand Detection**: mAP > 0.95
- **Landmark Accuracy**: Mean error < 5 pixels
- **Gesture Classification**: Accuracy > 95%
- **Inference Speed**: > 30 FPS on CPU

## ğŸ“– Documentation

Detailed documentation for each phase:
- [Phase 1: Data Collection Guide](1-data-collection/README.md)
- [Phase 2: Model Development Guide](2-model-development/README.md)
- [Phase 3: Application Guide](3-application/README.md)

## ğŸ¤ Use Cases

Beyond Spotify control, this model can be applied to:
- Gesture-based presentation control
- Sign language recognition
- Gaming control
- Smart home device control
- Accessibility applications

## Contributing

ChÃºng tÃ´i hoan nghÃªnh má»i Ä‘Ã³ng gÃ³p! / We welcome all contributions!

### Báº¯t Äáº§u Nhanh / Quick Start

1. **Fork** repository nÃ y
2. **Clone** vá» mÃ¡y cá»§a báº¡n
3. Táº¡o **virtual environment** vÃ  cÃ i Ä‘áº·t dependencies:
   ```bash
   python -m venv venv
   source venv/bin/activate  # macOS/Linux
   pip install -r requirements.txt
   ```
4. Copy `config/config.example.py` sang `config/config.py` vÃ  cáº¥u hÃ¬nh Spotify API
5. Táº¡o **branch má»›i** cho feature cá»§a báº¡n
6. Thá»±c hiá»‡n thay Ä‘á»•i vÃ  **commit**
7. **Push** lÃªn fork cá»§a báº¡n
8. Táº¡o **Pull Request**

### HÆ°á»›ng Dáº«n Chi Tiáº¿t / Detailed Guide

Äá»c hÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§ táº¡i [CONTRIBUTING.md](CONTRIBUTING.md) Ä‘á»ƒ biáº¿t:
- Quy trÃ¬nh phÃ¡t triá»ƒn chi tiáº¿t
- Code style guidelines
- CÃ¡ch viáº¿t tests
- Quy Æ°á»›c commit messages
- CÃ¡ch Ä‘á»“ng bá»™ vá»›i repository gá»‘c

### Ã TÆ°á»Ÿng ÄÃ³ng GÃ³p / Contribution Ideas

- ğŸ¯ ThÃªm gestures má»›i
- ğŸ› Sá»­a bugs trong Issues
- ğŸ“– Cáº£i thiá»‡n documentation
- âœ… Viáº¿t thÃªm tests
- âš¡ Cáº£i thiá»‡n performance

## License

MIT License

## Acknowledgments

- [MediaPipe](https://mediapipe.dev/) for hand detection
- [Spotipy](https://spotipy.readthedocs.io/) for Spotify API integration
